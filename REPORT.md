# White Agent for Web Navigation: Framework, Evaluation, and Results

## Abstract

We present an LLM-powered white agent for web navigation tasks that achieves 100% task success rate with an average of 2.3 steps per task, demonstrating 2-3x better efficiency than rule-based baselines. The agent uses a reactive architecture with chain-of-thought reasoning, receiving page observations (URL, title, DOM elements) and task instructions as input, and returning structured actions (click, type, select, scroll, wait, stop) with reasoning explanations as output. We evaluate the agent on Mind2Web-style tasks (finding product prices, ratings, and counts on a product catalog page) using six metrics: final_success (binary task completion), steps_taken (efficiency), trace_match_ratio (alignment with optimal solutions), wall_time_s (execution time), timeouts, and invalid_actions (reliability). The agent significantly outperforms the stub baseline (100% vs. 40-60% success rate, 2.3 vs. 7-10 steps) through semantic understanding that enables direct action selection without exploration, achieving trace_match_ratio of 0.83 and average completion time of 1.9 seconds per task.

## Benchmark

Web navigation has been a long-standing challenge in AI, with benchmarks evolving from simple form-filling tasks to complex multi-step web interactions. Early benchmarks like WebQA and WebShop focused on information extraction and e-commerce tasks, while more recent benchmarks like Mind2Web (Shi et al., 2023) introduced cross-website navigation tasks requiring agents to understand natural language instructions and interact with diverse web interfaces. Mind2Web evaluates agents on real-world websites with tasks like "find the price of a product" or "book a flight," measuring success through deterministic criteria (URL patterns, text matching, CSS selectors) and trace matching (comparing executed actions to gold standard solutions). Our benchmark follows Mind2Web's evaluation approach but focuses on controlled product catalog environments to enable reproducible evaluation and clear demonstration of agent capabilities.

Our benchmark evaluates white agents on web navigation tasks using a product catalog page with 5 products, each containing price, rating, and description elements. The benchmark includes three task types: element identification tasks ("Find the price of the third product"), semantic reasoning tasks ("Find the rating of the most expensive product"), and multi-step tasks ("Count the total number of products"). Each task includes a natural language instruction, starting URL, optional gold standard actions for trace matching, and deterministic success criteria (URL patterns, text regex patterns, CSS selector presence). The green agent evaluates white agents using six metrics: final_success (0/1 binary indicator of task completion based on success criteria), steps_taken (total actions executed), trace_match_ratio (0.0-1.0 proportion of actions matching gold standard), wall_time_s (total execution time), timeouts (number of timed-out actions), and invalid_actions (number of validation failures).

At each step, the white agent receives a natural language task description (e.g., "Find the price of the third product"), the current step number, and an observation containing the current page URL, page title, and a DOM summary of up to 50 interactive elements with their CSS selectors, HTML tags, text content, and element types. The agent also receives an action space specifying available action types (click, type, select, scroll, wait, stop). The agent returns a structured action dictionary with the action type and action-specific parameters (e.g., selector for click actions, text for type actions), along with a natural language explanation of reasoning in the thoughts field and metadata including confidence scores and model information.

The green agent evaluates white agents through a seven-step process. First, it extracts the current page state (URL, title, DOM summary) using Playwright. Second, it sends the observation and instruction to the white agent via HTTP POST to the `/act` endpoint. Third, it executes the returned action in the Playwright browser context. Fourth, it records the observation, action, and execution result in events.jsonl. Fifth, it repeats steps one through four until a stop action is received, max_steps is reached, or a timeout occurs. Sixth, after execution completes, it evaluates final_success by checking if all success criteria are met (URL contains specified substring, final HTML matches specified regex pattern, CSS selector exists in final HTML), computes trace_match_ratio by comparing executed actions to gold standard actions at each step, and calculates steps_taken, wall_time_s, timeouts, and invalid_actions. Seventh, it generates artifacts including logs, screenshots, Playwright traces, and structured events for analysis and debugging.

## White Agent Framework

The white agent uses a reactive LLM-based architecture with a single decision module that processes observations and returns actions at each step. The framework has no separate planner, executor, memory, verifier, or tool modulesâ€”instead, it relies on direct LLM reasoning with prompt engineering to guide decision-making. The agent is reactive (responds to current state rather than planning ahead) and stateless (each decision is independent, with no memory of previous steps beyond what's in the current observation). The design prioritizes simplicity and interpretability: all reasoning is captured in the thoughts field, making it easy to understand why each action was selected.

The decision-making pipeline consists of five stages executed sequentially at each step. First, the agent receives an HTTP POST request to the `/act` endpoint with instruction, step_idx, observation, and action_space. Second, the agent builds a structured prompt containing the task instruction, current page state (URL, title, step index), available DOM elements (up to 50, formatted with selector, tag, text, type), available action types and their formats, and chain-of-thought reasoning instructions that guide the LLM through systematic analysis (analyze task, identify element, choose action, use exact selector). Third, the agent calls the LLM (GPT-4o-mini or Claude 3.5 Haiku) with the prompt, requesting a JSON response with action, thoughts, and confidence fields. Fourth, the agent parses the LLM JSON response, validates action structure (checks required fields based on action type), and handles errors (returns stop action if parsing fails). Fifth, the agent returns a structured response with action, thoughts, and metadata. The pipeline uses chain-of-thought reasoning through prompt instructions that guide the LLM through systematic analysis, enabling the agent to produce interpretable reasoning without requiring separate reasoning modules.

At each step, the agent receives inputs including a natural language task description (e.g., "Find the price of the third product"), the current step number starting from 0, an observation dictionary containing the current page URL, page title, and a DOM summary list of up to 50 interactive elements (each with CSS selector, HTML tag, text content, and element type), and an action space dictionary specifying available action types. The agent returns outputs including an action dictionary with the selected action type (click, type, select, scroll, wait, stop) and action-specific fields (e.g., selector for click actions, text for type actions, delta_y for scroll actions, reason for stop actions), a thoughts string providing a natural language explanation of reasoning (e.g., "The task requires finding the price of the third product. I can see products numbered 1-5. The third product would be #product-3. I need to click on #product-3 .price"), and an info dictionary containing metadata such as confidence score, model name, and provider information.

The agent has a single module architecture with no separate components. The LLMWhiteAgent class contains all functionality: decide_action() orchestrates the pipeline, _build_prompt() constructs structured prompts from inputs, _call_openai() and _call_anthropic() call LLM APIs, and _parse_response() parses and validates LLM responses. The agent does not have separate modules for planning (it doesn't plan ahead, making reactive decisions), execution (execution is handled by the green agent using Playwright), memory (it is stateless with no memory of previous steps), verification (it relies on LLM reasoning without separate verification), or tools (it uses only LLM reasoning with no external tools). The agent interacts only with the green agent via HTTP: it receives POST requests with observations and returns JSON responses with actions. All reasoning happens internally through LLM calls, with no external tool usage or multi-agent coordination.

The agent uses chain-of-thought reasoning through prompt engineering: the prompt includes step-by-step instructions that guide the LLM through analysis ("1. Analyze the task instruction and current page state, 2. Identify the best element to interact with, 3. Choose the appropriate action type, 4. Use the exact CSS selector"). The agent does not use multi-step planning (it makes single-step decisions reactively), action refinement (it returns direct LLM output without refinement), program synthesis (no code generation or program execution), or tool-augmented reasoning (no external tools or APIs beyond the LLM). The reasoning is interpretable through the thoughts field, which explains each decision in natural language, allowing users to understand why the agent selected each action.
